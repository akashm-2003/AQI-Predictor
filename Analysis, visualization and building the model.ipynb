{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will print the outputs of only the first 5 entries for all arrays and lists else you guys have to scroll through a lot to reach the next cell and it also takes a lot of time to commit. If you want to print all the entries, you can fork this kernel and just run a loop until the length of the array or list and print all entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#importing the necessary libraries and dependencies\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m;\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m _dependency \u001b[39min\u001b[39;00m _hard_dependencies:\n\u001b[0;32m     10\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         \u001b[39m__import__\u001b[39;49m(_dependency)\n\u001b[0;32m     12\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m _e:\n\u001b[0;32m     13\u001b[0m         _missing_dependencies\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m_dependency\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m_e\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\__init__.py:153\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m polynomial\n\u001b[0;32m    152\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m random\n\u001b[1;32m--> 153\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ctypeslib\n\u001b[0;32m    154\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ma\n\u001b[0;32m    155\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m matrixlib \u001b[39mas\u001b[39;00m _mat\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\ctypeslib.py:376\u001b[0m\n\u001b[0;32m    367\u001b[0m     simple_types \u001b[39m=\u001b[39m [\n\u001b[0;32m    368\u001b[0m         ct\u001b[39m.\u001b[39mc_byte, ct\u001b[39m.\u001b[39mc_short, ct\u001b[39m.\u001b[39mc_int, ct\u001b[39m.\u001b[39mc_long, ct\u001b[39m.\u001b[39mc_longlong,\n\u001b[0;32m    369\u001b[0m         ct\u001b[39m.\u001b[39mc_ubyte, ct\u001b[39m.\u001b[39mc_ushort, ct\u001b[39m.\u001b[39mc_uint, ct\u001b[39m.\u001b[39mc_ulong, ct\u001b[39m.\u001b[39mc_ulonglong,\n\u001b[0;32m    370\u001b[0m         ct\u001b[39m.\u001b[39mc_float, ct\u001b[39m.\u001b[39mc_double,\n\u001b[0;32m    371\u001b[0m         ct\u001b[39m.\u001b[39mc_bool,\n\u001b[0;32m    372\u001b[0m     ]\n\u001b[0;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m {_dtype(ctype): ctype \u001b[39mfor\u001b[39;00m ctype \u001b[39min\u001b[39;00m simple_types}\n\u001b[1;32m--> 376\u001b[0m _scalar_type_map \u001b[39m=\u001b[39m _get_scalar_type_map()\n\u001b[0;32m    379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ctype_from_dtype_scalar\u001b[39m(dtype):\n\u001b[0;32m    380\u001b[0m     \u001b[39m# swapping twice ensure that `=` is promoted to <, >, or |\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     dtype_with_endian \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnewbyteorder(\u001b[39m'\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mnewbyteorder(\u001b[39m'\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\ctypeslib.py:373\u001b[0m, in \u001b[0;36m_get_scalar_type_map\u001b[1;34m()\u001b[0m\n\u001b[0;32m    366\u001b[0m ct \u001b[39m=\u001b[39m ctypes\n\u001b[0;32m    367\u001b[0m simple_types \u001b[39m=\u001b[39m [\n\u001b[0;32m    368\u001b[0m     ct\u001b[39m.\u001b[39mc_byte, ct\u001b[39m.\u001b[39mc_short, ct\u001b[39m.\u001b[39mc_int, ct\u001b[39m.\u001b[39mc_long, ct\u001b[39m.\u001b[39mc_longlong,\n\u001b[0;32m    369\u001b[0m     ct\u001b[39m.\u001b[39mc_ubyte, ct\u001b[39m.\u001b[39mc_ushort, ct\u001b[39m.\u001b[39mc_uint, ct\u001b[39m.\u001b[39mc_ulong, ct\u001b[39m.\u001b[39mc_ulonglong,\n\u001b[0;32m    370\u001b[0m     ct\u001b[39m.\u001b[39mc_float, ct\u001b[39m.\u001b[39mc_double,\n\u001b[0;32m    371\u001b[0m     ct\u001b[39m.\u001b[39mc_bool,\n\u001b[0;32m    372\u001b[0m ]\n\u001b[1;32m--> 373\u001b[0m \u001b[39mreturn\u001b[39;00m {_dtype(ctype): ctype \u001b[39mfor\u001b[39;49;00m ctype \u001b[39min\u001b[39;49;00m simple_types}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\ctypeslib.py:373\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    366\u001b[0m ct \u001b[39m=\u001b[39m ctypes\n\u001b[0;32m    367\u001b[0m simple_types \u001b[39m=\u001b[39m [\n\u001b[0;32m    368\u001b[0m     ct\u001b[39m.\u001b[39mc_byte, ct\u001b[39m.\u001b[39mc_short, ct\u001b[39m.\u001b[39mc_int, ct\u001b[39m.\u001b[39mc_long, ct\u001b[39m.\u001b[39mc_longlong,\n\u001b[0;32m    369\u001b[0m     ct\u001b[39m.\u001b[39mc_ubyte, ct\u001b[39m.\u001b[39mc_ushort, ct\u001b[39m.\u001b[39mc_uint, ct\u001b[39m.\u001b[39mc_ulong, ct\u001b[39m.\u001b[39mc_ulonglong,\n\u001b[0;32m    370\u001b[0m     ct\u001b[39m.\u001b[39mc_float, ct\u001b[39m.\u001b[39mc_double,\n\u001b[0;32m    371\u001b[0m     ct\u001b[39m.\u001b[39mc_bool,\n\u001b[0;32m    372\u001b[0m ]\n\u001b[1;32m--> 373\u001b[0m \u001b[39mreturn\u001b[39;00m {_dtype(ctype): ctype \u001b[39mfor\u001b[39;00m ctype \u001b[39min\u001b[39;00m simple_types}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_dtype_ctypes.py:114\u001b[0m, in \u001b[0;36mdtype_from_ctypes_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_ctypes_union(t)\n\u001b[0;32m    113\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39m_type_\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m), \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m _from_ctypes_scalar(t)\n\u001b[0;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    117\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnknown ctypes type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(t\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\_dtype_ctypes.py:76\u001b[0m, in \u001b[0;36m_from_ctypes_scalar\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39mReturn the dtype type with endianness included if it's the case\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39m__ctype_be__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m t:\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mdtype(\u001b[39m'\u001b[39m\u001b[39m>\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m t\u001b[39m.\u001b[39m_type_)\n\u001b[0;32m     77\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39m__ctype_le__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m t:\n\u001b[0;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m t\u001b[39m.\u001b[39m_type_)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "#importing the necessary libraries and dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns;\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# loading the data into the dataframe\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39ma.csv\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(df) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# loading the data into the dataframe\n",
    "df = pd.read_csv('a.csv') \n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing info about the columns\n",
    "df.info();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing few rows from the top\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows and columns in the dataset\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical information about columns\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how many null values are in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just doing `df.dropna()` drops all the NaN values only for the current execution of the cell. If you do the above `df.isnull().sum()` now, you can see that null values still persists. You can solve this by assigning the obtained output of \n",
    "`df.dopna()` to the variable `df` which stores our data (dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all the rows with NaN values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining training and testing data\n",
    "x_train = df[:24865]\n",
    "y_train = x_train['PM2.5']\n",
    "x_test = df[24865:31898]\n",
    "y_test = x_test['PM2.5']\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many pollutants. Let's first try to predict PM2.5 concentration values. Let the years 2016 and 2017 be the testing set. As you can see below, these 2 years account for 21.9% of the data (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[24865:31898].count() / df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing training data\n",
    "train_norm = x_train['PM2.5']\n",
    "\n",
    "#converted into array as all the methods available are for arrays and not lists\n",
    "train_norm_arr = np.asarray(train_norm)\n",
    "train_norm = np.reshape(train_norm_arr, (-1, 1))\n",
    "\n",
    "#Scaling all values between 0 and 1 so that large values don't just dominate\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_norm = scaler.fit_transform(train_norm)\n",
    "for i in range(5):\n",
    "    print(train_norm[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after normalization and scaing, null values are possible (many people disregard this). Let's check if any null values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(train_norm)):\n",
    "    if train_norm[i] == 0:\n",
    "        count = count +1\n",
    "print('Number of null values in train_norm = ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing null values \n",
    "train_norm = train_norm[train_norm!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing testing data and repeating the same process as done for training data\n",
    "test_norm = x_test['PM2.5']\n",
    "test_norm_arr = np.asarray(test_norm)\n",
    "test_norm = np.reshape(test_norm_arr, (-1, 1))\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "test_norm = scaler.fit_transform(test_norm)\n",
    "for i in range(5):\n",
    "    print(test_norm[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(test_norm)):\n",
    "    if test_norm[i] == 0:\n",
    "        count = count + 1 \n",
    "print('Number of null values in test_norm = ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing null values\n",
    "test_norm = test_norm[test_norm != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_norm.shape)\n",
    "print(test_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a time series data, we should be predicting the values after looking at a set of values rather than just a single value like we usually do. This takes into account the correlation between the data points and the timestamps. Because the neighbours should be considered for how the values change over time. Let's define a function to do this.\n",
    "\n",
    "The below function called split_sequence splits the sequence into sets of n values. This n is given as n_steps (step_size). For example, if n=3, we split the sequence in groups of 3. We create 2 empty lists and append the split sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X),array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the number of features = 1 as we will be predicting a single value. Let's reshape the split sequences into the format of number of rows, number of columns. (shape[0], shape[1]). In the output, we can see that groups of 3 since n_steps = 3 have been obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 3\n",
    "X_split_train, y_split_train = split_sequence(train_norm, n_steps)\n",
    "#for i in range(len(X_split_train)):\n",
    "    #print(X_split_train[i], y_split_train[i])\n",
    "n_features = 1\n",
    "X_split_train = X_split_train.reshape((X_split_train.shape[0], X_split_train.shape[1], n_features))\n",
    "for i in range(5):\n",
    "    print(X_split_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see below that, we predict the value for the first 3 values, then consider that output as one of the 3 values in the next set. For example, we preedict 0.1 first, then we take that 0.1 as input in the second set and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_split_test, y_split_test = split_sequence(test_norm, n_steps)\n",
    "for i in range(5):\n",
    "    print(X_split_test[i], y_split_test[i])\n",
    "n_features = 1\n",
    "X_split_test = X_split_test.reshape((X_split_test.shape[0], X_split_test.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our neural network (LSTM: Long Short Term Memory). Let's add 50 nodes in our first layer with a ReLU (Rectified linear unit) activation. Their shape will be step size, number of features. Then we will add, a dense layer with one node for the output.\n",
    "\n",
    "We can try out different optimizers to see which minimizes loss and maximizes accuracy. Stochastic gradient descent (SGD), Adam, AdaBoost, RMSProp are few of them. lr = learning rate, decay = by how much to decay the learning rate, momentum = how much should the gradient descent be accelerated to dampen oscillations, nesterov = whether to use nesterov momentum. Nesterov has stronger convergence for convex functions. And then we compile using MSE (mean squared loss) as our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.001, decay=1e-5, momentum=1.0, nesterov=False)\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.9, nesterov=True) #good\n",
    "\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01, rho=0.9)\n",
    "keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "hist = model.fit(X_split_train, y_split_train, validation_data=(X_split_test, y_split_test), epochs=10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_split_test)\n",
    "for i in range(5):\n",
    "    print(yhat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_split_test, yhat)\n",
    "print('MSE: %.5f' % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I have plotted the actual true values (first plot) and preedicted values (second plot). One can visually see that the distribution is almost the same. This says that our predictions are very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_split_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_acc = model.evaluate(X_split_train, y_split_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_split_test, y_split_test, verbose=0)\n",
    "print('Train: %.5f, Test: %.5f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, accuracy increase a lot in the last few epochs. Below, the loss gradually decrease. These are positive signs that our model is doing very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we just ran our model for prediction of a single pollutant. We have 6 pollutants in our dataset and can make predictions for all of them. So, I have made a function which can be used to predict the other pollutants rather than having to write the code again and again. I have commented the function calls. You can fork this kernel to uncomment and predit the other pollutants (Coz it would take up a lot of space and time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(var):\n",
    "    train_norm = x_train[var] \n",
    "    train_norm_arr = np.asarray(train_norm)\n",
    "    train_norm = np.reshape(train_norm_arr, (-1, 1))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_norm = scaler.fit_transform(train_norm)\n",
    "    train_norm = train_norm[train_norm != 0]\n",
    "    \n",
    "    test_norm = x_test[var]\n",
    "    test_norm_arr = np.asarray(test_norm)\n",
    "    test_norm = np.reshape(test_norm_arr, (-1, 1))\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    test_norm = scaler.fit_transform(test_norm)\n",
    "    test_norm = test_norm[test_norm != 0]\n",
    "\n",
    "    X_split_train, y_split_train = split_sequence(train_norm, n_steps)\n",
    "    X_split_train = X_split_train.reshape((X_split_train.shape[0], X_split_train.shape[1], n_features))\n",
    "\n",
    "    X_split_test, y_split_test = split_sequence(test_norm, n_steps)\n",
    "    X_split_test = X_split_test.reshape((X_split_test.shape[0], X_split_test.shape[1], n_features))\n",
    "\n",
    "    hist = model.fit(X_split_train, y_split_train, validation_data=(X_split_test, y_split_test), epochs=10, verbose = 1)\n",
    "\n",
    "    yhat = model.predict(X_split_test)\n",
    "\n",
    "    mse = mean_squared_error(y_split_test, yhat)\n",
    "    print(mse)\n",
    "    \n",
    "    plt.plot(hist.history['accuracy'])\n",
    "    plt.plot(hist.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute('PM10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute('SO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute('NO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute('CO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute('O3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will do a lot of visualizations to understand our data using various scatterplots, jointplots, pairplots, heatmap and correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=df['PM2.5'], y=df['PM10'], data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot gives us the idea that these two conentrations are positively correlated with very few outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding correlation\n",
    "corrmat = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(11,11))\n",
    "sns.heatmap(corrmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='density', subplots=True, layout=(4,4), sharex=False, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='PM2.5', y='PM10', c='DarkBlue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_split_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='PM10', y='SO2', c='DarkBlue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='SO2', y='NO2', c='DarkBlue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='NO2', y='CO', c='DarkBlue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='CO', y='O3', c='DarkBlue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap is a very useful visualization tool to know how much each feature is correlated. vmax = max value of the heatmap fmt = number of decimal places upto which the value is shown square = do you want the heatmap to be square shaped linewidth = width of the lines in the heatmap annot = should the boxes be labelled with the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
